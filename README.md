# ğŸ“š Document-Based AI Chatbot with LangChain + FAISS + Ollama + FastAPI

This is a simple, fast, and lightweight **RAG (Retrieval-Augmented Generation)** chatbot that answers questions **only using your own documents** (PDF or TXT). It uses:

- ğŸ§  [Ollama](https://ollama.com) for local LLM and embeddings (`phi3`)
- ğŸ” [FAISS](https://github.com/facebookresearch/faiss) for vector search
- âš™ï¸ [LangChain](https://www.langchain.com) for chaining logic
- ğŸš€ [FastAPI](https://fastapi.tiangolo.com) for RESTful API
- ğŸ³ Docker + Docker Compose for containerization

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py              # FastAPI app (query endpoint)
â”‚   â””â”€â”€ ingest.py           # Loads, chunks, embeds, and indexes documents
â”œâ”€â”€ docs/                   # Place your .pdf or .txt files here
â”‚   â””â”€â”€ privacy.txt
â”œâ”€â”€ faiss_index/            # FAISS vector index files
â”‚   â”œâ”€â”€ index.faiss
â”‚   â””â”€â”€ index.pkl
â”œâ”€â”€ Dockerfile              # Build image for chatbot
â”œâ”€â”€ docker-compose.yml      # Run Ollama + chatbot together
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # You are here
```

---

## âš™ï¸ How It Works

1. `ingest.py` loads all `.pdf` and `.txt` files from `/docs`, splits them into chunks, and embeds them using `phi3` via Ollama.
2. Embeddings are stored in a FAISS vector index.
3. `app.py` runs a FastAPI server with an `/ask` endpoint.
4. Incoming questions are answered **only using the documents** â€” if the answer is not found, it says:

```
"I don't know based on the provided information."
```

---

## ğŸš€ Quick Start

### 1. ğŸ§± Requirements

- Docker
- Docker Compose

### 2. ğŸ“‚ Add Your Documents

Place `.pdf` or `.txt` files inside the `docs/` folder:

```
docs/
â”œâ”€â”€ privacy.txt
â”œâ”€â”€ terms.pdf
```

### 3. ğŸ³ Run the App

```bash
docker compose up --build
```

- Ollama runs at `http://localhost:11434`
- Chatbot API runs at `http://localhost:8000`

---

## ğŸ“¡ Example API Usage

### POST `/ask`

```bash
curl -X POST http://localhost:8000/ask      -H "Content-Type: application/json"      -d '{"question": "What does the privacy policy say about cookies?"}'
```

### Example Response:

```json
{
  "answer": "The privacy policy mentions that cookies are used to improve user experience..."
}
```

If not found:

```json
{
  "answer": "I don't know based on the provided information."
}
```

---

## âœ… Environment Variables

| Variable         | Default               | Description              |
|------------------|------------------------|--------------------------|
| `OLLAMA_BASE_URL`| `http://ollama:11434`  | Ollama backend URL       |

---
## âš¡ï¸ GPU Support (optional)

If you have an NVIDIA GPU (e.g. 3090), you **must** use the `nvidia` runtime in Docker.

### 1. Install NVIDIA Docker Toolkit:

```bash
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```
---

## ğŸ“¦ Build Notes

Dockerfile builds the app and runs:

```bash
python ingest.py && uvicorn app:app --host 0.0.0.0 --port 8000
```

Ingesting happens **on container startup**, ensuring the latest documents are always indexed.

---

## ğŸ“„ License

Apache-2.0 license
---

## âœ¨ Credits

- [LangChain](https://github.com/langchain-ai/langchain)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Ollama](https://ollama.com)
- [FastAPI](https://fastapi.tiangolo.com)

---

## ğŸ™‹â€â™‚ï¸ Author

Built with â¤ï¸ by [Saeed Moradi]  
GitHub: [@sdmoradi](https://github.com/sdmoradi/)
